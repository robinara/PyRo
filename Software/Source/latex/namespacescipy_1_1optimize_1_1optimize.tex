\hypertarget{namespacescipy_1_1optimize_1_1optimize}{}\section{scipy.\+optimize.\+optimize Namespace Reference}
\label{namespacescipy_1_1optimize_1_1optimize}\index{scipy.\+optimize.\+optimize@{scipy.\+optimize.\+optimize}}
\subsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \hyperlink{classscipy_1_1optimize_1_1optimize_1_1__LineSearchError}{\+\_\+\+Line\+Search\+Error}
\item 
class \hyperlink{classscipy_1_1optimize_1_1optimize_1_1Brent}{Brent}
\item 
class \hyperlink{classscipy_1_1optimize_1_1optimize_1_1MemoizeJac}{Memoize\+Jac}
\item 
class \hyperlink{classscipy_1_1optimize_1_1optimize_1_1OptimizeResult}{Optimize\+Result}
\item 
class \hyperlink{classscipy_1_1optimize_1_1optimize_1_1OptimizeWarning}{Optimize\+Warning}
\end{DoxyCompactItemize}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_abcc9241a32d2a198c344be3500b40bed}{is\+\_\+array\+\_\+scalar}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a9241d7eaa432effbd6223507c9423972}{vecnorm}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a54798f84dfb5a20d037260e3adab6bdf}{rosen}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a43c4d14a97dce78cd7a21ab83ab2650d}{rosen\+\_\+der}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_ad424c93c37873405f553d088e991267c}{rosen\+\_\+hess}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a3752fb6e6fab99b132bf16e74b18bc61}{rosen\+\_\+hess\+\_\+prod}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a4109523b8106f6f5db40758bffbc8ec0}{wrap\+\_\+function}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_ad16a400c8d1388c70b0ed47b94c6a3d0}{fmin}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a9d6c794faeb5970fc92aac33b4e817c7}{approx\+\_\+fprime}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a7885f92532c0dbc6878f0f33364265c6}{check\+\_\+grad}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a9887ba75efa188c345723e0511e7e43e}{approx\+\_\+fhess\+\_\+p}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a114d58b070f13613387e7a33b268fc62}{fmin\+\_\+bfgs}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a77cb5c6679843a2823876599db1ab729}{fmin\+\_\+cg}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a3c84b9f84989d42e4f290b2f239cfb4e}{fmin\+\_\+ncg}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a8d39cb56630e984e55d771327422a19f}{fminbound}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_ad768f7b4799915afa21c8aecf8e878be}{brent}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_abf6e87803947c589e821c175f551dff2}{golden}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a3aa1746665030fc604388aa22db3e6dd}{bracket}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a62ccc9b2c6bfdcd2c168bd899dae8a17}{fmin\+\_\+powell}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_afbd077e695c6efe31abc98d5d4de687a}{brute}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_a979870d2383d7cff103f34be85182c9a}{show\+\_\+options}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1optimize_af85029d5f962caf6120e870266b3e89f}{main}
\end{DoxyCompactItemize}
\subsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \hyperlink{namespacescipy_1_1optimize_1_1optimize_a352ae7ae6c97270c980ee2ab5a79ff02}{\+\_\+\+\_\+all\+\_\+\+\_\+}
\item 
string \hyperlink{namespacescipy_1_1optimize_1_1optimize_a584735f307f25f012b46ea050e22f455}{\+\_\+\+\_\+docformat\+\_\+\+\_\+} = \char`\"{}restructuredtext en\char`\"{}
\item 
dictionary \hyperlink{namespacescipy_1_1optimize_1_1optimize_a7693ec03517c99592129c1db84464d17}{\+\_\+status\+\_\+message}
\item 
tuple \hyperlink{namespacescipy_1_1optimize_1_1optimize_a922ad0329a2cee476aa03d4d9abad53d}{\+\_\+epsilon} = \hyperlink{vecuops_8cc_ac9f82fdb8cd289615247f897852ee5f2}{sqrt}(numpy.\+finfo(float).eps)
\end{DoxyCompactItemize}


\subsection{Function Documentation}
\hypertarget{namespacescipy_1_1optimize_1_1optimize_a9887ba75efa188c345723e0511e7e43e}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!approx\+\_\+fhess\+\_\+p@{approx\+\_\+fhess\+\_\+p}}
\index{approx\+\_\+fhess\+\_\+p@{approx\+\_\+fhess\+\_\+p}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{approx\+\_\+fhess\+\_\+p}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+approx\+\_\+fhess\+\_\+p (
\begin{DoxyParamCaption}
\item[{}]{x0, }
\item[{}]{p, }
\item[{}]{fprime, }
\item[{}]{epsilon, }
\item[{}]{args}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a9887ba75efa188c345723e0511e7e43e}
\hypertarget{namespacescipy_1_1optimize_1_1optimize_a9d6c794faeb5970fc92aac33b4e817c7}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!approx\+\_\+fprime@{approx\+\_\+fprime}}
\index{approx\+\_\+fprime@{approx\+\_\+fprime}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{approx\+\_\+fprime}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+approx\+\_\+fprime (
\begin{DoxyParamCaption}
\item[{}]{xk, }
\item[{}]{f, }
\item[{}]{epsilon, }
\item[{}]{args}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a9d6c794faeb5970fc92aac33b4e817c7}
\begin{DoxyVerb}Finite-difference approximation of the gradient of a scalar function.

Parameters
----------
xk : array_like
    The coordinate vector at which to determine the gradient of `f`.
f : callable
    The function of which to determine the gradient (partial derivatives).
    Should take `xk` as first argument, other arguments to `f` can be
    supplied in ``*args``.  Should return a scalar, the value of the
    function at `xk`.
epsilon : array_like
    Increment to `xk` to use for determining the function gradient.
    If a scalar, uses the same finite difference delta for all partial
    derivatives.  If an array, should contain one value per element of
    `xk`.
\*args : args, optional
    Any other arguments that are to be passed to `f`.

Returns
-------
grad : ndarray
    The partial derivatives of `f` to `xk`.

See Also
--------
check_grad : Check correctness of gradient function against approx_fprime.

Notes
-----
The function gradient is determined by the forward finite difference
formula::

             f(xk[i] + epsilon[i]) - f(xk[i])
    f'[i] = ---------------------------------
                        epsilon[i]

The main use of `approx_fprime` is in scalar function optimizers like
`fmin_bfgs`, to determine numerically the Jacobian of a function.

Examples
--------
>>> from scipy import optimize
>>> def func(x, c0, c1):
...     "Coordinate vector `x` should be an array of size two."
...     return c0 * x[0]**2 + c1*x[1]**2

>>> x = np.ones(2)
>>> c0, c1 = (1, 200)
>>> eps = np.sqrt(np.finfo(np.float).eps)
>>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)
array([   2.        ,  400.00004198])\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a3aa1746665030fc604388aa22db3e6dd}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!bracket@{bracket}}
\index{bracket@{bracket}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{bracket}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+bracket (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{xa = {\ttfamily 0.0}, }
\item[{}]{xb = {\ttfamily 1.0}, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{grow\+\_\+limit = {\ttfamily 110.0}, }
\item[{}]{maxiter = {\ttfamily 1000}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a3aa1746665030fc604388aa22db3e6dd}
\begin{DoxyVerb}Bracket the minimum of the function.

Given a function and distinct initial points, search in the
downhill direction (as defined by the initital points) and return
new points xa, xb, xc that bracket the minimum of the function
f(xa) > f(xb) < f(xc). It doesn't always mean that obtained
solution will satisfy xa<=x<=xb

Parameters
----------
func : callable f(x,*args)
    Objective function to minimize.
xa, xb : float, optional
    Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.
args : tuple, optional
    Additional arguments (if present), passed to `func`.
grow_limit : float, optional
    Maximum grow limit.  Defaults to 110.0
maxiter : int, optional
    Maximum number of iterations to perform. Defaults to 1000.

Returns
-------
xa, xb, xc : float
    Bracket.
fa, fb, fc : float
    Objective function values in bracket.
funcalls : int
    Number of function evaluations made.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_ad768f7b4799915afa21c8aecf8e878be}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!brent@{brent}}
\index{brent@{brent}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{brent}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+brent (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{brack = {\ttfamily None}, }
\item[{}]{tol = {\ttfamily 1.48e-\/8}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{maxiter = {\ttfamily 500}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_ad768f7b4799915afa21c8aecf8e878be}
\begin{DoxyVerb}Given a function of one-variable and a possible bracketing interval,
return the minimum of the function isolated to a fractional precision of
tol.

Parameters
----------
func : callable f(x,*args)
    Objective function.
args
    Additional arguments (if present).
brack : tuple
    Triple (a,b,c) where (a<b<c) and func(b) <
    func(a),func(c).  If bracket consists of two numbers (a,c)
    then they are assumed to be a starting interval for a
    downhill bracket search (see `bracket`); it doesn't always
    mean that the obtained solution will satisfy a<=x<=c.
tol : float
    Stop if between iteration change is less than `tol`.
full_output : bool
    If True, return all output args (xmin, fval, iter,
    funcalls).
maxiter : int
    Maximum number of iterations in solution.

Returns
-------
xmin : ndarray
    Optimum point.
fval : float
    Optimum value.
iter : int
    Number of iterations.
funcalls : int
    Number of objective function evaluations made.

See also
--------
minimize_scalar: Interface to minimization algorithms for scalar
    univariate functions. See the 'Brent' `method` in particular.

Notes
-----
Uses inverse parabolic interpolation when possible to speed up
convergence of golden section method.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_afbd077e695c6efe31abc98d5d4de687a}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!brute@{brute}}
\index{brute@{brute}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{brute}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+brute (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{ranges, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{Ns = {\ttfamily 20}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{finish = {\ttfamily fmin}, }
\item[{}]{disp = {\ttfamily {\bf False}}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_afbd077e695c6efe31abc98d5d4de687a}
\begin{DoxyVerb}Minimize a function over a given range by brute force.

Uses the "brute force" method, i.e. computes the function's value
at each point of a multidimensional grid of points, to find the global
minimum of the function.

Parameters
----------
func : callable
    The objective function to be minimized. Must be in the
    form ``f(x, *args)``, where ``x`` is the argument in
    the form of a 1-D array and ``args`` is a tuple of any
    additional fixed parameters needed to completely specify
    the function.
ranges : tuple
    Each component of the `ranges` tuple must be either a
    "slice object" or a range tuple of the form ``(low, high)``.
    The program uses these to create the grid of points on which
    the objective function will be computed. See `Note 2` for
    more detail.
args : tuple, optional
    Any additional fixed parameters needed to completely specify
    the function.
Ns : int, optional
    Number of grid points along the axes, if not otherwise
    specified. See `Note2`.
full_output : bool, optional
    If True, return the evaluation grid and the objective function's
    values on it.
finish : callable, optional
    An optimization function that is called with the result of brute force
    minimization as initial guess.  `finish` should take the initial guess
    as positional argument, and take `args`, `full_output` and `disp`
    as keyword arguments.  Use None if no "polishing" function is to be
    used.  See Notes for more details.
disp : bool, optional
    Set to True to print convergence messages.

Returns
-------
x0 : ndarray
    A 1-D array containing the coordinates of a point at which the
    objective function had its minimum value. (See `Note 1` for
    which point is returned.)
fval : float
    Function value at the point `x0`.
grid : tuple
    Representation of the evaluation grid.  It has the same
    length as `x0`. (Returned when `full_output` is True.)
Jout : ndarray
    Function values at each point of the evaluation
    grid, `i.e.`, ``Jout = func(*grid)``. (Returned
    when `full_output` is True.)

See Also
--------
anneal : Another approach to seeking the global minimum of
multivariate, multimodal functions.

Notes
-----
*Note 1*: The program finds the gridpoint at which the lowest value
of the objective function occurs.  If `finish` is None, that is the
point returned.  When the global minimum occurs within (or not very far
outside) the grid's boundaries, and the grid is fine enough, that
point will be in the neighborhood of the gobal minimum.

However, users often employ some other optimization program to
"polish" the gridpoint values, `i.e.`, to seek a more precise
(local) minimum near `brute's` best gridpoint.
The `brute` function's `finish` option provides a convenient way to do
that.  Any polishing program used must take `brute's` output as its
initial guess as a positional argument, and take `brute's` input values
for `args` and `full_output` as keyword arguments, otherwise an error
will be raised.

`brute` assumes that the `finish` function returns a tuple in the form:
``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing value
of the argument, ``Jmin`` is the minimum value of the objective function,
"..." may be some other returned values (which are not used by `brute`),
and ``statuscode`` is the status code of the `finish` program.

Note that when `finish` is not None, the values returned are those
of the `finish` program, *not* the gridpoint ones.  Consequently,
while `brute` confines its search to the input grid points,
the `finish` program's results usually will not coincide with any
gridpoint, and may fall outside the grid's boundary.

*Note 2*: The grid of points is a `numpy.mgrid` object.
For `brute` the `ranges` and `Ns` inputs have the following effect.
Each component of the `ranges` tuple can be either a slice object or a
two-tuple giving a range of values, such as (0, 5).  If the component is a
slice object, `brute` uses it directly.  If the component is a two-tuple
range, `brute` internally converts it to a slice object that interpolates
`Ns` points from its low-value to its high-value, inclusive.

Examples
--------
We illustrate the use of `brute` to seek the global minimum of a function
of two variables that is given as the sum of a positive-definite
quadratic and two deep "Gaussian-shaped" craters.  Specifically, define
the objective function `f` as the sum of three other functions,
``f = f1 + f2 + f3``.  We suppose each of these has a signature
``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions
are as defined below.

>>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)
>>> def f1(z, *params):
...     x, y = z
...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params
...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)

>>> def f2(z, *params):
...     x, y = z
...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params
...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))

>>> def f3(z, *params):
...     x, y = z
...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params
...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))

>>> def f(z, *params):
...     x, y = z
...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params
...     return f1(z, *params) + f2(z, *params) + f3(z, *params)

Thus, the objective function may have local minima near the minimum
of each of the three functions of which it is composed.  To
use `fmin` to polish its gridpoint result, we may then continue as
follows:

>>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))
>>> from scipy import optimize
>>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,
                              finish=optimize.fmin)
>>> resbrute[0]  # global minimum
array([-1.05665192,  1.80834843])
>>> resbrute[1]  # function value at global minimum
-3.4085818767

Note that if `finish` had been set to None, we would have gotten the
gridpoint [-1.0 1.75] where the rounded function value is -2.892.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a7885f92532c0dbc6878f0f33364265c6}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!check\+\_\+grad@{check\+\_\+grad}}
\index{check\+\_\+grad@{check\+\_\+grad}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{check\+\_\+grad}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+check\+\_\+grad (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{grad, }
\item[{}]{x0, }
\item[{}]{args}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a7885f92532c0dbc6878f0f33364265c6}
\begin{DoxyVerb}Check the correctness of a gradient function by comparing it against a
(forward) finite-difference approximation of the gradient.

Parameters
----------
func : callable func(x0,*args)
    Function whose derivative is to be checked.
grad : callable grad(x0, *args)
    Gradient of `func`.
x0 : ndarray
    Points to check `grad` against forward difference approximation of grad
    using `func`.
args : \*args, optional
    Extra arguments passed to `func` and `grad`.

Returns
-------
err : float
    The square root of the sum of squares (i.e. the 2-norm) of the
    difference between ``grad(x0, *args)`` and the finite difference
    approximation of `grad` using func at the points `x0`.

See Also
--------
approx_fprime

Notes
-----
The step size used for the finite difference approximation is
`sqrt(numpy.finfo(float).eps)`, which is approximately 1.49e-08.

Examples
--------
>>> def func(x): return x[0]**2 - 0.5 * x[1]**3
>>> def grad(x): return [2 * x[0], -1.5 * x[1]**2]
>>> check_grad(func, grad, [1.5, -1.5])
2.9802322387695312e-08\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_ad16a400c8d1388c70b0ed47b94c6a3d0}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!fmin@{fmin}}
\index{fmin@{fmin}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{fmin}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+fmin (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{x0, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{xtol = {\ttfamily 1e-\/4}, }
\item[{}]{ftol = {\ttfamily 1e-\/4}, }
\item[{}]{maxiter = {\ttfamily None}, }
\item[{}]{maxfun = {\ttfamily None}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{disp = {\ttfamily 1}, }
\item[{}]{retall = {\ttfamily 0}, }
\item[{}]{callback = {\ttfamily None}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_ad16a400c8d1388c70b0ed47b94c6a3d0}
\begin{DoxyVerb}Minimize a function using the downhill simplex algorithm.

This algorithm only uses function values, not derivatives or second
derivatives.

Parameters
----------
func : callable func(x,*args)
    The objective function to be minimized.
x0 : ndarray
    Initial guess.
args : tuple, optional
    Extra arguments passed to func, i.e. ``f(x,*args)``.
callback : callable, optional
    Called after each iteration, as callback(xk), where xk is the
    current parameter vector.
xtol : float, optional
    Relative error in xopt acceptable for convergence.
ftol : number, optional
    Relative error in func(xopt) acceptable for convergence.
maxiter : int, optional
    Maximum number of iterations to perform.
maxfun : number, optional
    Maximum number of function evaluations to make.
full_output : bool, optional
    Set to True if fopt and warnflag outputs are desired.
disp : bool, optional
    Set to True to print convergence messages.
retall : bool, optional
    Set to True to return list of solutions at each iteration.

Returns
-------
xopt : ndarray
    Parameter that minimizes function.
fopt : float
    Value of function at minimum: ``fopt = func(xopt)``.
iter : int
    Number of iterations performed.
funcalls : int
    Number of function calls made.
warnflag : int
    1 : Maximum number of function evaluations made.
    2 : Maximum number of iterations reached.
allvecs : list
    Solution at each iteration.

See also
--------
minimize: Interface to minimization algorithms for multivariate
    functions. See the 'Nelder-Mead' `method` in particular.

Notes
-----
Uses a Nelder-Mead simplex algorithm to find the minimum of function of
one or more variables.

This algorithm has a long history of successful use in applications.
But it will usually be slower than an algorithm that uses first or
second derivative information. In practice it can have poor
performance in high-dimensional problems and is not robust to
minimizing complicated functions. Additionally, there currently is no
complete theory describing when the algorithm will successfully
converge to the minimum, or how fast it will if it does.

References
----------
.. [1] Nelder, J.A. and Mead, R. (1965), "A simplex method for function
       minimization", The Computer Journal, 7, pp. 308-313

.. [2] Wright, M.H. (1996), "Direct Search Methods: Once Scorned, Now
       Respectable", in Numerical Analysis 1995, Proceedings of the
       1995 Dundee Biennial Conference in Numerical Analysis, D.F.
       Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,
       Harlow, UK, pp. 191-208.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a114d58b070f13613387e7a33b268fc62}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!fmin\+\_\+bfgs@{fmin\+\_\+bfgs}}
\index{fmin\+\_\+bfgs@{fmin\+\_\+bfgs}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{fmin\+\_\+bfgs}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+fmin\+\_\+bfgs (
\begin{DoxyParamCaption}
\item[{}]{f, }
\item[{}]{x0, }
\item[{}]{fprime = {\ttfamily None}, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{gtol = {\ttfamily 1e-\/5}, }
\item[{}]{norm = {\ttfamily {\bf Inf}}, }
\item[{}]{epsilon = {\ttfamily \+\_\+epsilon}, }
\item[{}]{maxiter = {\ttfamily None}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{disp = {\ttfamily 1}, }
\item[{}]{retall = {\ttfamily 0}, }
\item[{}]{callback = {\ttfamily None}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a114d58b070f13613387e7a33b268fc62}
\begin{DoxyVerb}Minimize a function using the BFGS algorithm.

Parameters
----------
f : callable f(x,*args)
    Objective function to be minimized.
x0 : ndarray
    Initial guess.
fprime : callable f'(x,*args), optional
    Gradient of f.
args : tuple, optional
    Extra arguments passed to f and fprime.
gtol : float, optional
    Gradient norm must be less than gtol before successful termination.
norm : float, optional
    Order of norm (Inf is max, -Inf is min)
epsilon : int or ndarray, optional
    If fprime is approximated, use this value for the step size.
callback : callable, optional
    An optional user-supplied function to call after each
    iteration.  Called as callback(xk), where xk is the
    current parameter vector.
maxiter : int, optional
    Maximum number of iterations to perform.
full_output : bool, optional
    If True,return fopt, func_calls, grad_calls, and warnflag
    in addition to xopt.
disp : bool, optional
    Print convergence message if True.
retall : bool, optional
    Return a list of results at each iteration if True.

Returns
-------
xopt : ndarray
    Parameters which minimize f, i.e. f(xopt) == fopt.
fopt : float
    Minimum value.
gopt : ndarray
    Value of gradient at minimum, f'(xopt), which should be near 0.
Bopt : ndarray
    Value of 1/f''(xopt), i.e. the inverse hessian matrix.
func_calls : int
    Number of function_calls made.
grad_calls : int
    Number of gradient calls made.
warnflag : integer
    1 : Maximum number of iterations exceeded.
    2 : Gradient and/or function calls not changing.
allvecs  :  list
    `OptimizeResult` at each iteration.  Only returned if retall is True.

See also
--------
minimize: Interface to minimization algorithms for multivariate
    functions. See the 'BFGS' `method` in particular.

Notes
-----
Optimize the function, f, whose gradient is given by fprime
using the quasi-Newton method of Broyden, Fletcher, Goldfarb,
and Shanno (BFGS)

References
----------
Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a77cb5c6679843a2823876599db1ab729}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!fmin\+\_\+cg@{fmin\+\_\+cg}}
\index{fmin\+\_\+cg@{fmin\+\_\+cg}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{fmin\+\_\+cg}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+fmin\+\_\+cg (
\begin{DoxyParamCaption}
\item[{}]{f, }
\item[{}]{x0, }
\item[{}]{fprime = {\ttfamily None}, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{gtol = {\ttfamily 1e-\/5}, }
\item[{}]{norm = {\ttfamily {\bf Inf}}, }
\item[{}]{epsilon = {\ttfamily \+\_\+epsilon}, }
\item[{}]{maxiter = {\ttfamily None}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{disp = {\ttfamily 1}, }
\item[{}]{retall = {\ttfamily 0}, }
\item[{}]{callback = {\ttfamily None}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a77cb5c6679843a2823876599db1ab729}
\begin{DoxyVerb}Minimize a function using a nonlinear conjugate gradient algorithm.

Parameters
----------
f : callable, ``f(x, *args)``
    Objective function to be minimized.  Here `x` must be a 1-D array of
    the variables that are to be changed in the search for a minimum, and
    `args` are the other (fixed) parameters of `f`.
x0 : ndarray
    A user-supplied initial estimate of `xopt`, the optimal value of `x`.
    It must be a 1-D array of values.
fprime : callable, ``fprime(x, *args)``, optional
    A function that returns the gradient of `f` at `x`. Here `x` and `args`
    are as described above for `f`. The returned value must be a 1-D array.
    Defaults to None, in which case the gradient is approximated
    numerically (see `epsilon`, below).
args : tuple, optional
    Parameter values passed to `f` and `fprime`. Must be supplied whenever
    additional fixed parameters are needed to completely specify the
    functions `f` and `fprime`.
gtol : float, optional
    Stop when the norm of the gradient is less than `gtol`.
norm : float, optional
    Order to use for the norm of the gradient
    (``-np.Inf`` is min, ``np.Inf`` is max).
epsilon : float or ndarray, optional
    Step size(s) to use when `fprime` is approximated numerically. Can be a
    scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the
    floating point machine precision.  Usually ``sqrt(eps)`` is about
    1.5e-8.
maxiter : int, optional
    Maximum number of iterations to perform. Default is ``200 * len(x0)``.
full_output : bool, optional
    If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in
    addition to `xopt`.  See the Returns section below for additional
    information on optional return values.
disp : bool, optional
    If True, return a convergence message, followed by `xopt`.
retall : bool, optional
    If True, add to the returned values the results of each iteration.
callback : callable, optional
    An optional user-supplied function, called after each iteration.
    Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.

Returns
-------
xopt : ndarray
    Parameters which minimize f, i.e. ``f(xopt) == fopt``.
fopt : float, optional
    Minimum value found, f(xopt).  Only returned if `full_output` is True.
func_calls : int, optional
    The number of function_calls made.  Only returned if `full_output`
    is True.
grad_calls : int, optional
    The number of gradient calls made. Only returned if `full_output` is
    True.
warnflag : int, optional
    Integer value with warning status, only returned if `full_output` is
    True.

    0 : Success.

    1 : The maximum number of iterations was exceeded.

    2 : Gradient and/or function calls were not changing.  May indicate
        that precision was lost, i.e., the routine did not converge.

allvecs : list of ndarray, optional
    List of arrays, containing the results at each iteration.
    Only returned if `retall` is True.

See Also
--------
minimize : common interface to all `scipy.optimize` algorithms for
           unconstrained and constrained minimization of multivariate
           functions.  It provides an alternative way to call
           ``fmin_cg``, by specifying ``method='CG'``.

Notes
-----
This conjugate gradient algorithm is based on that of Polak and Ribiere
[1]_.

Conjugate gradient methods tend to work better when:

1. `f` has a unique global minimizing point, and no local minima or
   other stationary points,
2. `f` is, at least locally, reasonably well approximated by a
   quadratic function of the variables,
3. `f` is continuous and has a continuous gradient,
4. `fprime` is not too large, e.g., has a norm less than 1000,
5. The initial guess, `x0`, is reasonably close to `f` 's global
   minimizing point, `xopt`.

References
----------
.. [1] Wright & Nocedal, "Numerical Optimization", 1999, pp. 120-122.

Examples
--------
Example 1: seek the minimum value of the expression
``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values
of the parameters and an initial guess ``(u, v) = (0, 0)``.

>>> args = (2, 3, 7, 8, 9, 10)  # parameter values
>>> def f(x, *args):
...     u, v = x
...     a, b, c, d, e, f = args
...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f
>>> def gradf(x, *args):
...     u, v = x
...     a, b, c, d, e, f = args
...     gu = 2*a*u + b*v + d     # u-component of the gradient
...     gv = b*u + 2*c*v + e     # v-component of the gradient
...     return np.asarray((gu, gv))
>>> x0 = np.asarray((0, 0))  # Initial guess.
>>> from scipy import optimize
>>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)
>>> print 'res1 = ', res1
Optimization terminated successfully.
         Current function value: 1.617021
         Iterations: 2
         Function evaluations: 5
         Gradient evaluations: 5
res1 =  [-1.80851064 -0.25531915]

Example 2: solve the same problem using the `minimize` function.
(This `myopts` dictionary shows all of the available options,
although in practice only non-default values would be needed.
The returned value will be a dictionary.)

>>> opts = {'maxiter' : None,    # default value.
...         'disp' : True,    # non-default value.
...         'gtol' : 1e-5,    # default value.
...         'norm' : np.inf,  # default value.
...         'eps' : 1.4901161193847656e-08}  # default value.
>>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,
...                          method='CG', options=opts)
Optimization terminated successfully.
        Current function value: 1.617021
        Iterations: 2
        Function evaluations: 5
        Gradient evaluations: 5
>>> res2.x  # minimum found
array([-1.80851064 -0.25531915])\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a3c84b9f84989d42e4f290b2f239cfb4e}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!fmin\+\_\+ncg@{fmin\+\_\+ncg}}
\index{fmin\+\_\+ncg@{fmin\+\_\+ncg}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{fmin\+\_\+ncg}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+fmin\+\_\+ncg (
\begin{DoxyParamCaption}
\item[{}]{f, }
\item[{}]{x0, }
\item[{}]{fprime, }
\item[{}]{fhess\+\_\+p = {\ttfamily None}, }
\item[{}]{fhess = {\ttfamily None}, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{avextol = {\ttfamily 1e-\/5}, }
\item[{}]{epsilon = {\ttfamily \+\_\+epsilon}, }
\item[{}]{maxiter = {\ttfamily None}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{disp = {\ttfamily 1}, }
\item[{}]{retall = {\ttfamily 0}, }
\item[{}]{callback = {\ttfamily None}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a3c84b9f84989d42e4f290b2f239cfb4e}
\begin{DoxyVerb}Unconstrained minimization of a function using the Newton-CG method.

Parameters
----------
f : callable ``f(x, *args)``
    Objective function to be minimized.
x0 : ndarray
    Initial guess.
fprime : callable ``f'(x, *args)``
    Gradient of f.
fhess_p : callable ``fhess_p(x, p, *args)``, optional
    Function which computes the Hessian of f times an
    arbitrary vector, p.
fhess : callable ``fhess(x, *args)``, optional
    Function to compute the Hessian matrix of f.
args : tuple, optional
    Extra arguments passed to f, fprime, fhess_p, and fhess
    (the same set of extra arguments is supplied to all of
    these functions).
epsilon : float or ndarray, optional
    If fhess is approximated, use this value for the step size.
callback : callable, optional
    An optional user-supplied function which is called after
    each iteration.  Called as callback(xk), where xk is the
    current parameter vector.
avextol : float, optional
    Convergence is assumed when the average relative error in
    the minimizer falls below this amount.
maxiter : int, optional
    Maximum number of iterations to perform.
full_output : bool, optional
    If True, return the optional outputs.
disp : bool, optional
    If True, print convergence message.
retall : bool, optional
    If True, return a list of results at each iteration.

Returns
-------
xopt : ndarray
    Parameters which minimize f, i.e. ``f(xopt) == fopt``.
fopt : float
    Value of the function at xopt, i.e. ``fopt = f(xopt)``.
fcalls : int
    Number of function calls made.
gcalls : int
    Number of gradient calls made.
hcalls : int
    Number of hessian calls made.
warnflag : int
    Warnings generated by the algorithm.
    1 : Maximum number of iterations exceeded.
allvecs : list
    The result at each iteration, if retall is True (see below).

See also
--------
minimize: Interface to minimization algorithms for multivariate
    functions. See the 'Newton-CG' `method` in particular.

Notes
-----
Only one of `fhess_p` or `fhess` need to be given.  If `fhess`
is provided, then `fhess_p` will be ignored.  If neither `fhess`
nor `fhess_p` is provided, then the hessian product will be
approximated using finite differences on `fprime`. `fhess_p`
must compute the hessian times an arbitrary vector. If it is not
given, finite-differences on `fprime` are used to compute
it.

Newton-CG methods are also called truncated Newton methods. This
function differs from scipy.optimize.fmin_tnc because

1. scipy.optimize.fmin_ncg is written purely in python using numpy
    and scipy while scipy.optimize.fmin_tnc calls a C function.
2. scipy.optimize.fmin_ncg is only for unconstrained minimization
    while scipy.optimize.fmin_tnc is for unconstrained minimization
    or box constrained minimization. (Box constraints give
    lower and upper bounds for each variable separately.)

References
----------
Wright & Nocedal, 'Numerical Optimization', 1999, pg. 140.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a62ccc9b2c6bfdcd2c168bd899dae8a17}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!fmin\+\_\+powell@{fmin\+\_\+powell}}
\index{fmin\+\_\+powell@{fmin\+\_\+powell}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{fmin\+\_\+powell}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+fmin\+\_\+powell (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{x0, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{xtol = {\ttfamily 1e-\/4}, }
\item[{}]{ftol = {\ttfamily 1e-\/4}, }
\item[{}]{maxiter = {\ttfamily None}, }
\item[{}]{maxfun = {\ttfamily None}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{disp = {\ttfamily 1}, }
\item[{}]{retall = {\ttfamily 0}, }
\item[{}]{callback = {\ttfamily None}, }
\item[{}]{direc = {\ttfamily None}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a62ccc9b2c6bfdcd2c168bd899dae8a17}
\begin{DoxyVerb}Minimize a function using modified Powell's method. This method
only uses function values, not derivatives.

Parameters
----------
func : callable f(x,*args)
    Objective function to be minimized.
x0 : ndarray
    Initial guess.
args : tuple, optional
    Extra arguments passed to func.
callback : callable, optional
    An optional user-supplied function, called after each
    iteration.  Called as ``callback(xk)``, where ``xk`` is the
    current parameter vector.
direc : ndarray, optional
    Initial direction set.
xtol : float, optional
    Line-search error tolerance.
ftol : float, optional
    Relative error in ``func(xopt)`` acceptable for convergence.
maxiter : int, optional
    Maximum number of iterations to perform.
maxfun : int, optional
    Maximum number of function evaluations to make.
full_output : bool, optional
    If True, fopt, xi, direc, iter, funcalls, and
    warnflag are returned.
disp : bool, optional
    If True, print convergence messages.
retall : bool, optional
    If True, return a list of the solution at each iteration.

Returns
-------
xopt : ndarray
    Parameter which minimizes `func`.
fopt : number
    Value of function at minimum: ``fopt = func(xopt)``.
direc : ndarray
    Current direction set.
iter : int
    Number of iterations.
funcalls : int
    Number of function calls made.
warnflag : int
    Integer warning flag:
        1 : Maximum number of function evaluations.
        2 : Maximum number of iterations.
allvecs : list
    List of solutions at each iteration.

See also
--------
minimize: Interface to unconstrained minimization algorithms for
    multivariate functions. See the 'Powell' `method` in particular.

Notes
-----
Uses a modification of Powell's method to find the minimum of
a function of N variables. Powell's method is a conjugate
direction method.

The algorithm has two loops. The outer loop
merely iterates over the inner loop. The inner loop minimizes
over each current direction in the direction set. At the end
of the inner loop, if certain conditions are met, the direction
that gave the largest decrease is dropped and replaced with
the difference between the current estiamted x and the estimated
x from the beginning of the inner-loop.

The technical conditions for replacing the direction of greatest
increase amount to checking that

1. No further gain can be made along the direction of greatest increase
   from that iteration.
2. The direction of greatest increase accounted for a large sufficient
   fraction of the decrease in the function value from that iteration of
   the inner loop.

References
----------
Powell M.J.D. (1964) An efficient method for finding the minimum of a
function of several variables without calculating derivatives,
Computer Journal, 7 (2):155-162.

Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:
Numerical Recipes (any edition), Cambridge University Press\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a8d39cb56630e984e55d771327422a19f}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!fminbound@{fminbound}}
\index{fminbound@{fminbound}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{fminbound}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+fminbound (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{x1, }
\item[{}]{x2, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{xtol = {\ttfamily 1e-\/5}, }
\item[{}]{maxfun = {\ttfamily 500}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{disp = {\ttfamily 1}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a8d39cb56630e984e55d771327422a19f}
\begin{DoxyVerb}Bounded minimization for scalar functions.

Parameters
----------
func : callable f(x,*args)
    Objective function to be minimized (must accept and return scalars).
x1, x2 : float or array scalar
    The optimization bounds.
args : tuple, optional
    Extra arguments passed to function.
xtol : float, optional
    The convergence tolerance.
maxfun : int, optional
    Maximum number of function evaluations allowed.
full_output : bool, optional
    If True, return optional outputs.
disp : int, optional
    If non-zero, print messages.
        0 : no message printing.
        1 : non-convergence notification messages only.
        2 : print a message on convergence too.
        3 : print iteration results.


Returns
-------
xopt : ndarray
    Parameters (over given interval) which minimize the
    objective function.
fval : number
    The function value at the minimum point.
ierr : int
    An error flag (0 if converged, 1 if maximum number of
    function calls reached).
numfunc : int
  The number of function calls made.

See also
--------
minimize_scalar: Interface to minimization algorithms for scalar
    univariate functions. See the 'Bounded' `method` in particular.

Notes
-----
Finds a local minimizer of the scalar function `func` in the
interval x1 < xopt < x2 using Brent's method.  (See `brent`
for auto-bracketing).\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_abf6e87803947c589e821c175f551dff2}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!golden@{golden}}
\index{golden@{golden}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{golden}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+golden (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{brack = {\ttfamily None}, }
\item[{}]{tol = {\ttfamily \+\_\+epsilon}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_abf6e87803947c589e821c175f551dff2}
\begin{DoxyVerb}Return the minimum of a function of one variable.

Given a function of one variable and a possible bracketing interval,
return the minimum of the function isolated to a fractional precision of
tol.

Parameters
----------
func : callable func(x,*args)
    Objective function to minimize.
args : tuple
    Additional arguments (if present), passed to func.
brack : tuple
    Triple (a,b,c), where (a<b<c) and func(b) <
    func(a),func(c).  If bracket consists of two numbers (a,
    c), then they are assumed to be a starting interval for a
    downhill bracket search (see `bracket`); it doesn't always
    mean that obtained solution will satisfy a<=x<=c.
tol : float
    x tolerance stop criterion
full_output : bool
    If True, return optional outputs.

See also
--------
minimize_scalar: Interface to minimization algorithms for scalar
    univariate functions. See the 'Golden' `method` in particular.

Notes
-----
Uses analog of bisection method to decrease the bracketed
interval.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_abcc9241a32d2a198c344be3500b40bed}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!is\+\_\+array\+\_\+scalar@{is\+\_\+array\+\_\+scalar}}
\index{is\+\_\+array\+\_\+scalar@{is\+\_\+array\+\_\+scalar}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{is\+\_\+array\+\_\+scalar}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+is\+\_\+array\+\_\+scalar (
\begin{DoxyParamCaption}
\item[{}]{x}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_abcc9241a32d2a198c344be3500b40bed}
\begin{DoxyVerb}Test whether `x` is either a scalar or an array scalar.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_af85029d5f962caf6120e870266b3e89f}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!main@{main}}
\index{main@{main}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{main}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+main (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_af85029d5f962caf6120e870266b3e89f}
\hypertarget{namespacescipy_1_1optimize_1_1optimize_a54798f84dfb5a20d037260e3adab6bdf}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!rosen@{rosen}}
\index{rosen@{rosen}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{rosen}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+rosen (
\begin{DoxyParamCaption}
\item[{}]{x}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a54798f84dfb5a20d037260e3adab6bdf}
\begin{DoxyVerb}The Rosenbrock function.

The function computed is::

    sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0

Parameters
----------
x : array_like
    1-D array of points at which the Rosenbrock function is to be computed.

Returns
-------
f : float
    The value of the Rosenbrock function.

See Also
--------
rosen_der, rosen_hess, rosen_hess_prod\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a43c4d14a97dce78cd7a21ab83ab2650d}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!rosen\+\_\+der@{rosen\+\_\+der}}
\index{rosen\+\_\+der@{rosen\+\_\+der}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{rosen\+\_\+der}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+rosen\+\_\+der (
\begin{DoxyParamCaption}
\item[{}]{x}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a43c4d14a97dce78cd7a21ab83ab2650d}
\begin{DoxyVerb}The derivative (i.e. gradient) of the Rosenbrock function.

Parameters
----------
x : array_like
    1-D array of points at which the derivative is to be computed.

Returns
-------
rosen_der : (N,) ndarray
    The gradient of the Rosenbrock function at `x`.

See Also
--------
rosen, rosen_hess, rosen_hess_prod\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_ad424c93c37873405f553d088e991267c}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!rosen\+\_\+hess@{rosen\+\_\+hess}}
\index{rosen\+\_\+hess@{rosen\+\_\+hess}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{rosen\+\_\+hess}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+rosen\+\_\+hess (
\begin{DoxyParamCaption}
\item[{}]{x}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_ad424c93c37873405f553d088e991267c}
\begin{DoxyVerb}The Hessian matrix of the Rosenbrock function.

Parameters
----------
x : array_like
    1-D array of points at which the Hessian matrix is to be computed.

Returns
-------
rosen_hess : ndarray
    The Hessian matrix of the Rosenbrock function at `x`.

See Also
--------
rosen, rosen_der, rosen_hess_prod\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a3752fb6e6fab99b132bf16e74b18bc61}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!rosen\+\_\+hess\+\_\+prod@{rosen\+\_\+hess\+\_\+prod}}
\index{rosen\+\_\+hess\+\_\+prod@{rosen\+\_\+hess\+\_\+prod}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{rosen\+\_\+hess\+\_\+prod}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+rosen\+\_\+hess\+\_\+prod (
\begin{DoxyParamCaption}
\item[{}]{x, }
\item[{}]{p}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a3752fb6e6fab99b132bf16e74b18bc61}
\begin{DoxyVerb}Product of the Hessian matrix of the Rosenbrock function with a vector.

Parameters
----------
x : array_like
    1-D array of points at which the Hessian matrix is to be computed.
p : array_like
    1-D array, the vector to be multiplied by the Hessian matrix.

Returns
-------
rosen_hess_prod : ndarray
    The Hessian matrix of the Rosenbrock function at `x` multiplied
    by the vector `p`.

See Also
--------
rosen, rosen_der, rosen_hess\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a979870d2383d7cff103f34be85182c9a}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!show\+\_\+options@{show\+\_\+options}}
\index{show\+\_\+options@{show\+\_\+options}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{show\+\_\+options}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+show\+\_\+options (
\begin{DoxyParamCaption}
\item[{}]{solver = {\ttfamily None}, }
\item[{}]{method = {\ttfamily None}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a979870d2383d7cff103f34be85182c9a}
\begin{DoxyVerb}Show documentation for additional options of optimization solvers.

These are method-specific options that can be supplied through the
``options`` dict.

Parameters
----------
solver : str
    Type of optimization solver. One of 'minimize', 'minimize_scalar', 'root'.
method : str, optional
    If not given, shows all methods of the specified solver. Otherwise,
    show only the options for the specified method. Valid values
    corresponds to methods' names of respective solver (e.g. 'BFGS' for
    'minimize').

Notes
-----

**Minimize options**

*BFGS* options:

    gtol : float
        Gradient norm must be less than `gtol` before successful
        termination.
    norm : float
        Order of norm (Inf is max, -Inf is min).
    eps : float or ndarray
        If `jac` is approximated, use this value for the step size.

*Nelder-Mead* options:

    xtol : float
        Relative error in solution `xopt` acceptable for convergence.
    ftol : float
        Relative error in ``fun(xopt)`` acceptable for convergence.
    maxfev : int
        Maximum number of function evaluations to make.

*Newton-CG* options:

    xtol : float
        Average relative error in solution `xopt` acceptable for
        convergence.
    eps : float or ndarray
        If `jac` is approximated, use this value for the step size.

*CG* options:

    gtol : float
        Gradient norm must be less than `gtol` before successful
        termination.
    norm : float
        Order of norm (Inf is max, -Inf is min).
    eps : float or ndarray
        If `jac` is approximated, use this value for the step size.

*Powell* options:

    xtol : float
        Relative error in solution `xopt` acceptable for convergence.
    ftol : float
        Relative error in ``fun(xopt)`` acceptable for convergence.
    maxfev : int
        Maximum number of function evaluations to make.
    direc : ndarray
        Initial set of direction vectors for the Powell method.

*Anneal* options:

    ftol : float
        Relative error in ``fun(x)`` acceptable for convergence.
    schedule : str
        Annealing schedule to use. One of: 'fast', 'cauchy' or
        'boltzmann'.
    T0 : float
        Initial Temperature (estimated as 1.2 times the largest
        cost-function deviation over random points in the range).
    Tf : float
        Final goal temperature.
    maxfev : int
        Maximum number of function evaluations to make.
    maxaccept : int
        Maximum changes to accept.
    boltzmann : float
        Boltzmann constant in acceptance test (increase for less
        stringent test at each temperature).
    learn_rate : float
        Scale constant for adjusting guesses.
    quench, m, n : float
        Parameters to alter fast_sa schedule.
    lower, upper : float or ndarray
        Lower and upper bounds on `x`.
    dwell : int
        The number of times to search the space at each temperature.

*L-BFGS-B* options:

    ftol : float
        The iteration stops when ``(f^k -
        f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.
    gtol : float
        The iteration will stop when ``max{|proj g_i | i = 1, ..., n}
        <= gtol`` where ``pg_i`` is the i-th component of the
        projected gradient.
    maxcor : int
        The maximum number of variable metric corrections used to
        define the limited memory matrix. (The limited memory BFGS
        method does not store the full hessian but uses this many terms
        in an approximation to it.)
    maxiter : int
        Maximum number of function evaluations.

*TNC* options:

    ftol : float
        Precision goal for the value of f in the stoping criterion.
        If ftol < 0.0, ftol is set to 0.0 defaults to -1.
    xtol : float
        Precision goal for the value of x in the stopping
        criterion (after applying x scaling factors).  If xtol <
        0.0, xtol is set to sqrt(machine_precision).  Defaults to
        -1.
    gtol : float
        Precision goal for the value of the projected gradient in
        the stopping criterion (after applying x scaling factors).
        If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).
        Setting it to 0.0 is not recommended.  Defaults to -1.
    scale : list of floats
        Scaling factors to apply to each variable.  If None, the
        factors are up-low for interval bounded variables and
        1+|x] fo the others.  Defaults to None
    offset : float
        Value to subtract from each variable.  If None, the
        offsets are (up+low)/2 for interval bounded variables
        and x for the others.
    maxCGit : int
        Maximum number of hessian*vector evaluations per main
        iteration.  If maxCGit == 0, the direction chosen is
        -gradient if maxCGit < 0, maxCGit is set to
        max(1,min(50,n/2)).  Defaults to -1.
    maxiter : int
        Maximum number of function evaluation.  if None, `maxiter` is
        set to max(100, 10*len(x0)).  Defaults to None.
    eta : float
        Severity of the line search. if < 0 or > 1, set to 0.25.
        Defaults to -1.
    stepmx : float
        Maximum step for the line search.  May be increased during
        call.  If too small, it will be set to 10.0.  Defaults to 0.
    accuracy : float
        Relative precision for finite difference calculations.  If
        <= machine_precision, set to sqrt(machine_precision).
        Defaults to 0.
    minfev : float
        Minimum function value estimate.  Defaults to 0.
    rescale : float
        Scaling factor (in log10) used to trigger f value
        rescaling.  If 0, rescale at each iteration.  If a large
        value, never rescale.  If < 0, rescale is set to 1.3.

*COBYLA* options:

    tol : float
        Final accuracy in the optimization (not precisely guaranteed).
        This is a lower bound on the size of the trust region.
    rhobeg : float
        Reasonable initial changes to the variables.
    maxfev : int
        Maximum number of function evaluations.
    catol : float
        Absolute tolerance for constraint violations (default: 1e-6).

*SLSQP* options:

    ftol : float
        Precision goal for the value of f in the stopping criterion.
    eps : float
        Step size used for numerical approximation of the jacobian.
    maxiter : int
        Maximum number of iterations.

*dogleg* options:

    initial_trust_radius : float
        Initial trust-region radius.
    max_trust_radius : float
        Maximum value of the trust-region radius. No steps that are longer
        than this value will be proposed.
    eta : float
        Trust region related acceptance stringency for proposed steps.
    gtol : float
        Gradient norm must be less than `gtol` before successful
        termination.

*trust-ncg* options:

    See dogleg options.


**minimize_scalar options**

*brent* options:

    xtol : float

        Relative error in solution `xopt` acceptable for convergence.

*bounded* options:

    xatol : float
        Absolute error in solution `xopt` acceptable for convergence.

*golden* options:

    xtol : float
        Relative error in solution `xopt` acceptable for convergence.


**root options**

*hybrd* options:

    col_deriv : bool
        Specify whether the Jacobian function computes derivatives down
        the columns (faster, because there is no transpose operation).
    xtol : float
        The calculation will terminate if the relative error between
        two consecutive iterates is at most `xtol`.
    maxfev : int
        The maximum number of calls to the function. If zero, then
        ``100*(N+1)`` is the maximum where N is the number of elements
        in `x0`.
    band : sequence
        If set to a two-sequence containing the number of sub- and
        super-diagonals within the band of the Jacobi matrix, the
        Jacobi matrix is considered banded (only for ``fprime=None``).
    epsfcn : float
        A suitable step length for the forward-difference approximation
        of the Jacobian (for ``fprime=None``). If `epsfcn` is less than
        the machine precision, it is assumed that the relative errors
        in the functions are of the order of the machine precision.
    factor : float
        A parameter determining the initial step bound (``factor * ||
        diag * x||``).  Should be in the interval ``(0.1, 100)``.
    diag : sequence
        N positive entries that serve as a scale factors for the
        variables.

*LM* options:

    col_deriv : bool
        non-zero to specify that the Jacobian function computes derivatives
        down the columns (faster, because there is no transpose operation).
    ftol : float
        Relative error desired in the sum of squares.
    xtol : float
        Relative error desired in the approximate solution.
    gtol : float
        Orthogonality desired between the function vector and the columns
        of the Jacobian.
    maxiter : int
        The maximum number of calls to the function. If zero, then
        100*(N+1) is the maximum where N is the number of elements in x0.
    epsfcn : float
        A suitable step length for the forward-difference approximation of
        the Jacobian (for Dfun=None). If epsfcn is less than the machine
        precision, it is assumed that the relative errors in the functions
        are of the order of the machine precision.
    factor : float
        A parameter determining the initial step bound
        (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.
    diag : sequence
        N positive entries that serve as a scale factors for the variables.

*Broyden1* options:

    nit : int, optional
        Number of iterations to make. If omitted (default), make as many
        as required to meet tolerances.
    disp : bool, optional
        Print status to stdout on every iteration.
    maxiter : int, optional
        Maximum number of iterations to make. If more are needed to
        meet convergence, `NoConvergence` is raised.
    ftol : float, optional
        Relative tolerance for the residual. If omitted, not used.
    fatol : float, optional
        Absolute tolerance (in max-norm) for the residual.
        If omitted, default is 6e-6.
    xtol : float, optional
        Relative minimum step size. If omitted, not used.
    xatol : float, optional
        Absolute minimum step size, as determined from the Jacobian
        approximation. If the step size is smaller than this, optimization
        is terminated as successful. If omitted, not used.
    tol_norm : function(vector) -> scalar, optional
        Norm to use in convergence check. Default is the maximum norm.
    line_search : {None, 'armijo' (default), 'wolfe'}, optional
        Which type of a line search to use to determine the step size in
        the direction given by the Jacobian approximation. Defaults to
        'armijo'.
    jac_options : dict, optional
        Options for the respective Jacobian approximation.
            alpha : float, optional
                Initial guess for the Jacobian is (-1/alpha).
            reduction_method : str or tuple, optional
                Method used in ensuring that the rank of the Broyden
                matrix stays low. Can either be a string giving the
                name of the method, or a tuple of the form ``(method,
                param1, param2, ...)`` that gives the name of the
                method and values for additional parameters.

                Methods available:
                    - ``restart``: drop all matrix columns. Has no
                        extra parameters.
                    - ``simple``: drop oldest matrix column. Has no
                        extra parameters.
                    - ``svd``: keep only the most significant SVD
                        components.
                      Extra parameters:
                          - ``to_retain`: number of SVD components to
                              retain when rank reduction is done.
                              Default is ``max_rank - 2``.
            max_rank : int, optional
                Maximum rank for the Broyden matrix.
                Default is infinity (ie., no rank reduction).

*Broyden2* options:

    nit : int, optional
        Number of iterations to make. If omitted (default), make as many
        as required to meet tolerances.
    disp : bool, optional
        Print status to stdout on every iteration.
    maxiter : int, optional
        Maximum number of iterations to make. If more are needed to
        meet convergence, `NoConvergence` is raised.
    ftol : float, optional
        Relative tolerance for the residual. If omitted, not used.
    fatol : float, optional
        Absolute tolerance (in max-norm) for the residual.
        If omitted, default is 6e-6.
    xtol : float, optional
        Relative minimum step size. If omitted, not used.
    xatol : float, optional
        Absolute minimum step size, as determined from the Jacobian
        approximation. If the step size is smaller than this, optimization
        is terminated as successful. If omitted, not used.
    tol_norm : function(vector) -> scalar, optional
        Norm to use in convergence check. Default is the maximum norm.
    line_search : {None, 'armijo' (default), 'wolfe'}, optional
        Which type of a line search to use to determine the step size in
        the direction given by the Jacobian approximation. Defaults to
        'armijo'.
    jac_options : dict, optional
        Options for the respective Jacobian approximation.

        alpha : float, optional
            Initial guess for the Jacobian is (-1/alpha).
        reduction_method : str or tuple, optional
            Method used in ensuring that the rank of the Broyden
            matrix stays low. Can either be a string giving the
            name of the method, or a tuple of the form ``(method,
            param1, param2, ...)`` that gives the name of the
            method and values for additional parameters.

            Methods available:
                - ``restart``: drop all matrix columns. Has no
                    extra parameters.
                - ``simple``: drop oldest matrix column. Has no
                    extra parameters.
                - ``svd``: keep only the most significant SVD
                    components.
                  Extra parameters:
                      - ``to_retain`: number of SVD components to
                          retain when rank reduction is done.
                          Default is ``max_rank - 2``.
        max_rank : int, optional
            Maximum rank for the Broyden matrix.
            Default is infinity (ie., no rank reduction).

*Anderson* options:

    nit : int, optional
        Number of iterations to make. If omitted (default), make as many
        as required to meet tolerances.
    disp : bool, optional
        Print status to stdout on every iteration.
    maxiter : int, optional
        Maximum number of iterations to make. If more are needed to
        meet convergence, `NoConvergence` is raised.
    ftol : float, optional
        Relative tolerance for the residual. If omitted, not used.
    fatol : float, optional
        Absolute tolerance (in max-norm) for the residual.
        If omitted, default is 6e-6.
    xtol : float, optional
        Relative minimum step size. If omitted, not used.
    xatol : float, optional
        Absolute minimum step size, as determined from the Jacobian
        approximation. If the step size is smaller than this, optimization
        is terminated as successful. If omitted, not used.
    tol_norm : function(vector) -> scalar, optional
        Norm to use in convergence check. Default is the maximum norm.
    line_search : {None, 'armijo' (default), 'wolfe'}, optional
        Which type of a line search to use to determine the step size in
        the direction given by the Jacobian approximation. Defaults to
        'armijo'.
    jac_options : dict, optional
        Options for the respective Jacobian approximation.

        alpha : float, optional
            Initial guess for the Jacobian is (-1/alpha).
        M : float, optional
            Number of previous vectors to retain. Defaults to 5.
        w0 : float, optional
            Regularization parameter for numerical stability.
            Compared to unity, good values of the order of 0.01.

*LinearMixing* options:

    nit : int, optional
        Number of iterations to make. If omitted (default), make as many
        as required to meet tolerances.
    disp : bool, optional
        Print status to stdout on every iteration.
    maxiter : int, optional
        Maximum number of iterations to make. If more are needed to
        meet convergence, `NoConvergence` is raised.
    ftol : float, optional
        Relative tolerance for the residual. If omitted, not used.
    fatol : float, optional
        Absolute tolerance (in max-norm) for the residual.
        If omitted, default is 6e-6.
    xtol : float, optional
        Relative minimum step size. If omitted, not used.
    xatol : float, optional
        Absolute minimum step size, as determined from the Jacobian
        approximation. If the step size is smaller than this, optimization
        is terminated as successful. If omitted, not used.
    tol_norm : function(vector) -> scalar, optional
        Norm to use in convergence check. Default is the maximum norm.
    line_search : {None, 'armijo' (default), 'wolfe'}, optional
        Which type of a line search to use to determine the step size in
        the direction given by the Jacobian approximation. Defaults to
        'armijo'.
    jac_options : dict, optional
        Options for the respective Jacobian approximation.

        alpha : float, optional
            initial guess for the jacobian is (-1/alpha).

*DiagBroyden* options:

    nit : int, optional
        Number of iterations to make. If omitted (default), make as many
        as required to meet tolerances.
    disp : bool, optional
        Print status to stdout on every iteration.
    maxiter : int, optional
        Maximum number of iterations to make. If more are needed to
        meet convergence, `NoConvergence` is raised.
    ftol : float, optional
        Relative tolerance for the residual. If omitted, not used.
    fatol : float, optional
        Absolute tolerance (in max-norm) for the residual.
        If omitted, default is 6e-6.
    xtol : float, optional
        Relative minimum step size. If omitted, not used.
    xatol : float, optional
        Absolute minimum step size, as determined from the Jacobian
        approximation. If the step size is smaller than this, optimization
        is terminated as successful. If omitted, not used.
    tol_norm : function(vector) -> scalar, optional
        Norm to use in convergence check. Default is the maximum norm.
    line_search : {None, 'armijo' (default), 'wolfe'}, optional
        Which type of a line search to use to determine the step size in
        the direction given by the Jacobian approximation. Defaults to
        'armijo'.
    jac_options : dict, optional
        Options for the respective Jacobian approximation.

        alpha : float, optional
            initial guess for the jacobian is (-1/alpha).

*ExcitingMixing* options:

    nit : int, optional
        Number of iterations to make. If omitted (default), make as many
        as required to meet tolerances.
    disp : bool, optional
        Print status to stdout on every iteration.
    maxiter : int, optional
        Maximum number of iterations to make. If more are needed to
        meet convergence, `NoConvergence` is raised.
    ftol : float, optional
        Relative tolerance for the residual. If omitted, not used.
    fatol : float, optional
        Absolute tolerance (in max-norm) for the residual.
        If omitted, default is 6e-6.
    xtol : float, optional
        Relative minimum step size. If omitted, not used.
    xatol : float, optional
        Absolute minimum step size, as determined from the Jacobian
        approximation. If the step size is smaller than this, optimization
        is terminated as successful. If omitted, not used.
    tol_norm : function(vector) -> scalar, optional
        Norm to use in convergence check. Default is the maximum norm.
    line_search : {None, 'armijo' (default), 'wolfe'}, optional
        Which type of a line search to use to determine the step size in
        the direction given by the Jacobian approximation. Defaults to
        'armijo'.
    jac_options : dict, optional
        Options for the respective Jacobian approximation.

        alpha : float, optional
            Initial Jacobian approximation is (-1/alpha).
        alphamax : float, optional
            The entries of the diagonal Jacobian are kept in the range
            ``[alpha, alphamax]``.

*Krylov* options:

    nit : int, optional
        Number of iterations to make. If omitted (default), make as many
        as required to meet tolerances.
    disp : bool, optional
        Print status to stdout on every iteration.
    maxiter : int, optional
        Maximum number of iterations to make. If more are needed to
        meet convergence, `NoConvergence` is raised.
    ftol : float, optional
        Relative tolerance for the residual. If omitted, not used.
    fatol : float, optional
        Absolute tolerance (in max-norm) for the residual.
        If omitted, default is 6e-6.
    xtol : float, optional
        Relative minimum step size. If omitted, not used.
    xatol : float, optional
        Absolute minimum step size, as determined from the Jacobian
        approximation. If the step size is smaller than this, optimization
        is terminated as successful. If omitted, not used.
    tol_norm : function(vector) -> scalar, optional
        Norm to use in convergence check. Default is the maximum norm.
    line_search : {None, 'armijo' (default), 'wolfe'}, optional
        Which type of a line search to use to determine the step size in
        the direction given by the Jacobian approximation. Defaults to
        'armijo'.
    jac_options : dict, optional
        Options for the respective Jacobian approximation.

        rdiff : float, optional
            Relative step size to use in numerical differentiation.
        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function
            Krylov method to use to approximate the Jacobian.
            Can be a string, or a function implementing the same
            interface as the iterative solvers in
            `scipy.sparse.linalg`.

            The default is `scipy.sparse.linalg.lgmres`.
        inner_M : LinearOperator or InverseJacobian
            Preconditioner for the inner Krylov iteration.
            Note that you can use also inverse Jacobians as (adaptive)
            preconditioners. For example,

            >>> jac = BroydenFirst()
            >>> kjac = KrylovJacobian(inner_M=jac.inverse).

            If the preconditioner has a method named 'update', it will
            be called as ``update(x, f)`` after each nonlinear step,
            with ``x`` giving the current point, and ``f`` the current
            function value.
        inner_tol, inner_maxiter, ...
            Parameters to pass on to the "inner" Krylov solver.
            See `scipy.sparse.linalg.gmres` for details.
        outer_k : int, optional
            Size of the subspace kept across LGMRES nonlinear
            iterations.

            See `scipy.sparse.linalg.lgmres` for details.\end{DoxyVerb}
 \hypertarget{namespacescipy_1_1optimize_1_1optimize_a9241d7eaa432effbd6223507c9423972}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!vecnorm@{vecnorm}}
\index{vecnorm@{vecnorm}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{vecnorm}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+vecnorm (
\begin{DoxyParamCaption}
\item[{}]{x, }
\item[{}]{ord = {\ttfamily 2}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a9241d7eaa432effbd6223507c9423972}
\hypertarget{namespacescipy_1_1optimize_1_1optimize_a4109523b8106f6f5db40758bffbc8ec0}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!wrap\+\_\+function@{wrap\+\_\+function}}
\index{wrap\+\_\+function@{wrap\+\_\+function}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{wrap\+\_\+function}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+optimize.\+wrap\+\_\+function (
\begin{DoxyParamCaption}
\item[{}]{function, }
\item[{}]{args}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1optimize_a4109523b8106f6f5db40758bffbc8ec0}


\subsection{Variable Documentation}
\hypertarget{namespacescipy_1_1optimize_1_1optimize_a352ae7ae6c97270c980ee2ab5a79ff02}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!\+\_\+\+\_\+all\+\_\+\+\_\+@{\+\_\+\+\_\+all\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+all\+\_\+\+\_\+@{\+\_\+\+\_\+all\+\_\+\+\_\+}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{\+\_\+\+\_\+all\+\_\+\+\_\+}]{\setlength{\rightskip}{0pt plus 5cm}list scipy.\+optimize.\+optimize.\+\_\+\+\_\+all\+\_\+\+\_\+}\label{namespacescipy_1_1optimize_1_1optimize_a352ae7ae6c97270c980ee2ab5a79ff02}
{\bfseries Initial value\+:}
\begin{DoxyCode}
1 = [\textcolor{stringliteral}{'fmin'}, \textcolor{stringliteral}{'fmin\_powell'}, \textcolor{stringliteral}{'fmin\_bfgs'}, \textcolor{stringliteral}{'fmin\_ncg'}, \textcolor{stringliteral}{'fmin\_cg'},
2            \textcolor{stringliteral}{'fminbound'}, \textcolor{stringliteral}{'brent'}, \textcolor{stringliteral}{'golden'}, \textcolor{stringliteral}{'bracket'}, \textcolor{stringliteral}{'rosen'}, \textcolor{stringliteral}{'rosen\_der'},
3            \textcolor{stringliteral}{'rosen\_hess'}, \textcolor{stringliteral}{'rosen\_hess\_prod'}, \textcolor{stringliteral}{'brute'}, \textcolor{stringliteral}{'approx\_fprime'},
4            \textcolor{stringliteral}{'line\_search'}, \textcolor{stringliteral}{'check\_grad'}, \textcolor{stringliteral}{'OptimizeResult'}, \textcolor{stringliteral}{'show\_options'},
5            \textcolor{stringliteral}{'OptimizeWarning'}]
\end{DoxyCode}
\hypertarget{namespacescipy_1_1optimize_1_1optimize_a584735f307f25f012b46ea050e22f455}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!\+\_\+\+\_\+docformat\+\_\+\+\_\+@{\+\_\+\+\_\+docformat\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+docformat\+\_\+\+\_\+@{\+\_\+\+\_\+docformat\+\_\+\+\_\+}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{\+\_\+\+\_\+docformat\+\_\+\+\_\+}]{\setlength{\rightskip}{0pt plus 5cm}string scipy.\+optimize.\+optimize.\+\_\+\+\_\+docformat\+\_\+\+\_\+ = \char`\"{}restructuredtext en\char`\"{}}\label{namespacescipy_1_1optimize_1_1optimize_a584735f307f25f012b46ea050e22f455}
\hypertarget{namespacescipy_1_1optimize_1_1optimize_a922ad0329a2cee476aa03d4d9abad53d}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!\+\_\+epsilon@{\+\_\+epsilon}}
\index{\+\_\+epsilon@{\+\_\+epsilon}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{\+\_\+epsilon}]{\setlength{\rightskip}{0pt plus 5cm}tuple scipy.\+optimize.\+optimize.\+\_\+epsilon = {\bf sqrt}(numpy.\+finfo(float).eps)}\label{namespacescipy_1_1optimize_1_1optimize_a922ad0329a2cee476aa03d4d9abad53d}
\hypertarget{namespacescipy_1_1optimize_1_1optimize_a7693ec03517c99592129c1db84464d17}{}\index{scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}!\+\_\+status\+\_\+message@{\+\_\+status\+\_\+message}}
\index{\+\_\+status\+\_\+message@{\+\_\+status\+\_\+message}!scipy\+::optimize\+::optimize@{scipy\+::optimize\+::optimize}}
\subsubsection[{\+\_\+status\+\_\+message}]{\setlength{\rightskip}{0pt plus 5cm}dictionary scipy.\+optimize.\+optimize.\+\_\+status\+\_\+message}\label{namespacescipy_1_1optimize_1_1optimize_a7693ec03517c99592129c1db84464d17}
{\bfseries Initial value\+:}
\begin{DoxyCode}
1 = \{\textcolor{stringliteral}{'success'}: \textcolor{stringliteral}{'Optimization terminated successfully.'},
2                    \textcolor{stringliteral}{'maxfev'}: \textcolor{stringliteral}{'Maximum number of function evaluations has '}
3                               \textcolor{stringliteral}{'been exceeded.'},
4                    \textcolor{stringliteral}{'maxiter'}: \textcolor{stringliteral}{'Maximum number of iterations has been '}
5                               \textcolor{stringliteral}{'exceeded.'},
6                    \textcolor{stringliteral}{'pr\_loss'}: \textcolor{stringliteral}{'Desired error not necessarily achieved due '}
7                               \textcolor{stringliteral}{'to precision loss.'}\}
\end{DoxyCode}
