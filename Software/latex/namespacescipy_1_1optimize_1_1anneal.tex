\hypertarget{namespacescipy_1_1optimize_1_1anneal}{}\section{scipy.\+optimize.\+anneal Namespace Reference}
\label{namespacescipy_1_1optimize_1_1anneal}\index{scipy.\+optimize.\+anneal@{scipy.\+optimize.\+anneal}}
\subsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \hyperlink{classscipy_1_1optimize_1_1anneal_1_1__state}{\+\_\+state}
\item 
class \hyperlink{classscipy_1_1optimize_1_1anneal_1_1base__schedule}{base\+\_\+schedule}
\item 
class \hyperlink{classscipy_1_1optimize_1_1anneal_1_1boltzmann__sa}{boltzmann\+\_\+sa}
\item 
class \hyperlink{classscipy_1_1optimize_1_1anneal_1_1cauchy__sa}{cauchy\+\_\+sa}
\item 
class \hyperlink{classscipy_1_1optimize_1_1anneal_1_1fast__sa}{fast\+\_\+sa}
\end{DoxyCompactItemize}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{namespacescipy_1_1optimize_1_1anneal_a6e856bcbe0ef0a2074e01b790f5dd815}{anneal}
\end{DoxyCompactItemize}
\subsection*{Variables}
\begin{DoxyCompactItemize}
\item 
list \hyperlink{namespacescipy_1_1optimize_1_1anneal_a7b74d0f5e2ba2cd3719916a14ee26ab2}{\+\_\+\+\_\+all\+\_\+\+\_\+} = \mbox{[}'\hyperlink{namespacescipy_1_1optimize_1_1anneal_a6e856bcbe0ef0a2074e01b790f5dd815}{anneal}'\mbox{]}
\item 
tuple \hyperlink{namespacescipy_1_1optimize_1_1anneal_af5e528ec388b7b92412aadde93e8c4b8}{\+\_\+double\+\_\+min} = numpy.\+finfo(float)
\item 
tuple \hyperlink{namespacescipy_1_1optimize_1_1anneal_acbff336abdcf5645b0c7aaef918776d4}{\+\_\+double\+\_\+max} = numpy.\+finfo(float)
\item 
tuple \hyperlink{namespacescipy_1_1optimize_1_1anneal_a3661d82424de8745ae3e1fa2124c7d86}{func} = lambdax\+:cos(14.\+5 $\ast$ \hyperlink{vecnorm1_8cc_ac73eed9e41ec09d58f112f06c2d6cb63}{x} -\/ 0.\+3)
\item 
int \hyperlink{namespacescipy_1_1optimize_1_1anneal_a2afdf374faad6006f3d7b99f788be82b}{feps} = 1
\item 
list \hyperlink{namespacescipy_1_1optimize_1_1anneal_a043335cd5334b15f26ffea18942a2149}{upper} = \mbox{[}3.\+0, 3.\+0\mbox{]}
\end{DoxyCompactItemize}


\subsection{Function Documentation}
\hypertarget{namespacescipy_1_1optimize_1_1anneal_a6e856bcbe0ef0a2074e01b790f5dd815}{}\index{scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}!anneal@{anneal}}
\index{anneal@{anneal}!scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}}
\subsubsection[{anneal}]{\setlength{\rightskip}{0pt plus 5cm}def scipy.\+optimize.\+anneal.\+anneal (
\begin{DoxyParamCaption}
\item[{}]{func, }
\item[{}]{x0, }
\item[{}]{args = {\ttfamily ()}, }
\item[{}]{schedule = {\ttfamily 'fast'}, }
\item[{}]{full\+\_\+output = {\ttfamily 0}, }
\item[{}]{T0 = {\ttfamily None}, }
\item[{}]{Tf = {\ttfamily 1e-\/12}, }
\item[{}]{maxeval = {\ttfamily None}, }
\item[{}]{maxaccept = {\ttfamily None}, }
\item[{}]{maxiter = {\ttfamily 400}, }
\item[{}]{boltzmann = {\ttfamily 1.0}, }
\item[{}]{learn\+\_\+rate = {\ttfamily 0.5}, }
\item[{}]{feps = {\ttfamily 1e-\/6}, }
\item[{}]{quench = {\ttfamily 1.0}, }
\item[{}]{m = {\ttfamily 1.0}, }
\item[{}]{n = {\ttfamily 1.0}, }
\item[{}]{lower = {\ttfamily -\/100}, }
\item[{}]{upper = {\ttfamily 100}, }
\item[{}]{dwell = {\ttfamily 50}, }
\item[{}]{disp = {\ttfamily {\bf True}}}
\end{DoxyParamCaption}
)}\label{namespacescipy_1_1optimize_1_1anneal_a6e856bcbe0ef0a2074e01b790f5dd815}
\begin{DoxyVerb}Minimize a function using simulated annealing.

Uses simulated annealing, a random algorithm that uses no derivative
information from the function being optimized. Other names for this
family of approaches include: "Monte Carlo", "Metropolis",
"Metropolis-Hastings", `etc`. They all involve (a) evaluating the
objective function on a random set of points, (b) keeping those that
pass their randomized evaluation critera, (c) cooling (`i.e.`,
tightening) the evaluation critera, and (d) repeating until their
termination critera are met.  In practice they have been used mainly in
discrete rather than in continuous optimization.

Available annealing schedules are 'fast', 'cauchy' and 'boltzmann'.

Parameters
----------
func : callable
    The objective function to be minimized.  Must be in the form
    `f(x, *args)`, where `x` is the argument in the form of a 1-D array
    and `args` is a  tuple of any additional fixed parameters needed to
    completely specify the function.
x0: 1-D array
    An initial guess at the optimizing argument of `func`.
args : tuple, optional
    Any additional fixed parameters needed to completely
    specify the objective function.
schedule : str, optional
    The annealing schedule to use.  Must be one of 'fast', 'cauchy' or
    'boltzmann'.  See `Notes`.
full_output : bool, optional
    If `full_output`, then return all values listed in the Returns
    section. Otherwise, return just the `xmin` and `status` values.
T0 : float, optional
    The initial "temperature".  If None, then estimate it as 1.2 times
    the largest cost-function deviation over random points in the
    box-shaped region specified by the `lower, upper` input parameters.
Tf : float, optional
    Final goal temperature.  Cease iterations if the temperature
    falls below `Tf`.
maxeval : int, optional
    Cease iterations if the number of function evaluations exceeds
    `maxeval`.
maxaccept : int, optional
    Cease iterations if the number of points accepted exceeds `maxaccept`.
    See `Notes` for the probabilistic acceptance criteria used.
maxiter : int, optional
    Cease iterations if the number of cooling iterations exceeds `maxiter`.
learn_rate : float, optional
    Scale constant for tuning the probabilistc acceptance criteria.
boltzmann : float, optional
    Boltzmann constant in the probabilistic acceptance criteria
    (increase for less stringent criteria at each temperature).
feps : float, optional
    Cease iterations if the relative errors in the function value over the
    last four coolings is below `feps`.
quench, m, n : floats, optional
    Parameters to alter the `fast` simulated annealing schedule.
    See `Notes`.
lower, upper : floats or 1-D arrays, optional
    Lower and upper bounds on the argument `x`.  If floats are provided,
    they apply to all components of `x`.
dwell : int, optional
    The number of times to execute the inner loop at each value of the
    temperature.  See `Notes`.
disp : bool, optional
    Print a descriptive convergence message if True.

Returns
-------
xmin : ndarray
    The point where the lowest function value was found.
Jmin : float
    The objective function value at `xmin`.
T : float
    The temperature at termination of the iterations.
feval : int
    Number of function evaluations used.
iters : int
    Number of cooling iterations used.
accept : int
    Number of tests accepted.
status : int
    A code indicating the reason for termination:

    - 0 : Points no longer changing.
    - 1 : Cooled to final temperature.
    - 2 : Maximum function evaluations reached.
    - 3 : Maximum cooling iterations reached.
    - 4 : Maximum accepted query locations reached.
    - 5 : Final point not the minimum amongst encountered points.

See Also
--------
basinhopping : another (more performant) global optimizer
brute : brute-force global optimizer

Notes
-----
Simulated annealing is a random algorithm which uses no derivative
information from the function being optimized. In practice it has
been more useful in discrete optimization than continuous
optimization, as there are usually better algorithms for continuous
optimization problems.

Some experimentation by trying the different temperature
schedules and altering their parameters is likely required to
obtain good performance.

The randomness in the algorithm comes from random sampling in numpy.
To obtain the same results you can call `numpy.random.seed` with the
same seed immediately before calling `anneal`.

We give a brief description of how the three temperature schedules
generate new points and vary their temperature.  Temperatures are
only updated with iterations in the outer loop.  The inner loop is
over loop over ``xrange(dwell)``, and new points are generated for
every iteration in the inner loop.  Whether the proposed new points
are accepted is probabilistic.

For readability, let ``d`` denote the dimension of the inputs to func.
Also, let ``x_old`` denote the previous state, and ``k`` denote the
iteration number of the outer loop.  All other variables not
defined below are input variables to `anneal` itself.

In the 'fast' schedule the updates are::

    u ~ Uniform(0, 1, size = d)
    y = sgn(u - 0.5) * T * ((1 + 1/T)**abs(2*u - 1) - 1.0)

    xc = y * (upper - lower)
    x_new = x_old + xc

    c = n * exp(-n * quench)
    T_new = T0 * exp(-c * k**quench)

In the 'cauchy' schedule the updates are::

    u ~ Uniform(-pi/2, pi/2, size=d)
    xc = learn_rate * T * tan(u)
    x_new = x_old + xc

    T_new = T0 / (1 + k)

In the 'boltzmann' schedule the updates are::

    std = minimum(sqrt(T) * ones(d), (upper - lower) / (3*learn_rate))
    y ~ Normal(0, std, size = d)
    x_new = x_old + learn_rate * y

    T_new = T0 / log(1 + k)

References
----------
[1] P. J. M. van Laarhoven and E. H. L. Aarts, "Simulated Annealing: Theory
    and Applications", Kluwer Academic Publishers, 1987.

[2] W.H. Press et al., "Numerical Recipies: The Art of Scientific Computing",
    Cambridge U. Press, 1987.

Examples
--------
*Example 1.* We illustrate the use of `anneal` to seek the global minimum
of a function of two variables that is equal to the sum of a positive-
definite quadratic and two deep "Gaussian-shaped" craters.  Specifically,
define the objective function `f` as the sum of three other functions,
``f = f1 + f2 + f3``.  We suppose each of these has a signature
``(z, *params)``, where ``z = (x, y)``, ``params``, and the functions are
as defined below.

>>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)
>>> def f1(z, *params):
...     x, y = z
...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params
...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)

>>> def f2(z, *params):
...     x, y = z
...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params
...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))

>>> def f3(z, *params):
...     x, y = z
...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params
...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))

>>> def f(z, *params):
...     x, y = z
...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params
...     return f1(z, *params) + f2(z, *params) + f3(z, *params)

>>> x0 = np.array([2., 2.])     # Initial guess.
>>> from scipy import optimize
>>> np.random.seed(555)   # Seeded to allow replication.
>>> res = optimize.anneal(f, x0, args=params, schedule='boltzmann',
                          full_output=True, maxiter=500, lower=-10,
                          upper=10, dwell=250, disp=True)
Warning: Maximum number of iterations exceeded.
>>> res[0]  # obtained minimum
array([-1.03914194,  1.81330654])
>>> res[1]  # function value at minimum
-3.3817...

So this run settled on the point [-1.039, 1.813] with a minimum function
value of about -3.382.  The final temperature was about 212. The run used
125301 function evaluations, 501 iterations (including the initial guess as
a iteration), and accepted 61162 points. The status flag of 3 also
indicates that `maxiter` was reached.

This problem's true global minimum lies near the point [-1.057, 1.808]
and has a value of about -3.409.  So these `anneal` results are pretty
good and could be used as the starting guess in a local optimizer to
seek a more exact local minimum.

*Example 2.* To minimize the same objective function using
the `minimize` approach, we need to (a) convert the options to an
"options dictionary" using the keys prescribed for this method,
(b) call the `minimize` function with the name of the method (which
in this case is 'Anneal'), and (c) take account of the fact that
the returned value will be a `OptimizeResult` object (`i.e.`, a dictionary,
as defined in `optimize.py`).

All of the allowable options for 'Anneal' when using the `minimize`
approach are listed in the ``myopts`` dictionary given below, although
in practice only the non-default values would be needed.  Some of their
names differ from those used in the `anneal` approach.  We can proceed
as follows:

>>> myopts = {
        'schedule'     : 'boltzmann',   # Non-default value.
        'maxfev'       : None,  # Default, formerly `maxeval`.
        'maxiter'      : 500,   # Non-default value.
        'maxaccept'    : None,  # Default value.
        'ftol'         : 1e-6,  # Default, formerly `feps`.
        'T0'           : None,  # Default value.
        'Tf'           : 1e-12, # Default value.
        'boltzmann'    : 1.0,   # Default value.
        'learn_rate'   : 0.5,   # Default value.
        'quench'       : 1.0,   # Default value.
        'm'            : 1.0,   # Default value.
        'n'            : 1.0,   # Default value.
        'lower'        : -10,   # Non-default value.
        'upper'        : +10,   # Non-default value.
        'dwell'        : 250,   # Non-default value.
        'disp'         : True   # Default value.
        }
>>> from scipy import optimize
>>> np.random.seed(777)  # Seeded to allow replication.
>>> res2 = optimize.minimize(f, x0, args=params, method='Anneal',
                             options=myopts)
Warning: Maximum number of iterations exceeded.
>>> res2
  status: 3
 success: False
  accept: 61742
    nfev: 125301
       T: 214.20624873839623
     fun: -3.4084065576676053
       x: array([-1.05757366,  1.8071427 ])
 message: 'Maximum cooling iterations reached'
 nit: 501\end{DoxyVerb}
 

\subsection{Variable Documentation}
\hypertarget{namespacescipy_1_1optimize_1_1anneal_a7b74d0f5e2ba2cd3719916a14ee26ab2}{}\index{scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}!\+\_\+\+\_\+all\+\_\+\+\_\+@{\+\_\+\+\_\+all\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+all\+\_\+\+\_\+@{\+\_\+\+\_\+all\+\_\+\+\_\+}!scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}}
\subsubsection[{\+\_\+\+\_\+all\+\_\+\+\_\+}]{\setlength{\rightskip}{0pt plus 5cm}list scipy.\+optimize.\+anneal.\+\_\+\+\_\+all\+\_\+\+\_\+ = \mbox{[}'{\bf anneal}'\mbox{]}}\label{namespacescipy_1_1optimize_1_1anneal_a7b74d0f5e2ba2cd3719916a14ee26ab2}
\hypertarget{namespacescipy_1_1optimize_1_1anneal_acbff336abdcf5645b0c7aaef918776d4}{}\index{scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}!\+\_\+double\+\_\+max@{\+\_\+double\+\_\+max}}
\index{\+\_\+double\+\_\+max@{\+\_\+double\+\_\+max}!scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}}
\subsubsection[{\+\_\+double\+\_\+max}]{\setlength{\rightskip}{0pt plus 5cm}tuple scipy.\+optimize.\+anneal.\+\_\+double\+\_\+max = numpy.\+finfo(float)}\label{namespacescipy_1_1optimize_1_1anneal_acbff336abdcf5645b0c7aaef918776d4}
\hypertarget{namespacescipy_1_1optimize_1_1anneal_af5e528ec388b7b92412aadde93e8c4b8}{}\index{scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}!\+\_\+double\+\_\+min@{\+\_\+double\+\_\+min}}
\index{\+\_\+double\+\_\+min@{\+\_\+double\+\_\+min}!scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}}
\subsubsection[{\+\_\+double\+\_\+min}]{\setlength{\rightskip}{0pt plus 5cm}tuple scipy.\+optimize.\+anneal.\+\_\+double\+\_\+min = numpy.\+finfo(float)}\label{namespacescipy_1_1optimize_1_1anneal_af5e528ec388b7b92412aadde93e8c4b8}
\hypertarget{namespacescipy_1_1optimize_1_1anneal_a2afdf374faad6006f3d7b99f788be82b}{}\index{scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}!feps@{feps}}
\index{feps@{feps}!scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}}
\subsubsection[{feps}]{\setlength{\rightskip}{0pt plus 5cm}int scipy.\+optimize.\+anneal.\+feps = 1}\label{namespacescipy_1_1optimize_1_1anneal_a2afdf374faad6006f3d7b99f788be82b}
\hypertarget{namespacescipy_1_1optimize_1_1anneal_a3661d82424de8745ae3e1fa2124c7d86}{}\index{scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}!func@{func}}
\index{func@{func}!scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}}
\subsubsection[{func}]{\setlength{\rightskip}{0pt plus 5cm}tuple scipy.\+optimize.\+anneal.\+func = lambdax\+:cos(14.\+5 $\ast$ {\bf x} -\/ 0.\+3)}\label{namespacescipy_1_1optimize_1_1anneal_a3661d82424de8745ae3e1fa2124c7d86}
\hypertarget{namespacescipy_1_1optimize_1_1anneal_a043335cd5334b15f26ffea18942a2149}{}\index{scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}!upper@{upper}}
\index{upper@{upper}!scipy\+::optimize\+::anneal@{scipy\+::optimize\+::anneal}}
\subsubsection[{upper}]{\setlength{\rightskip}{0pt plus 5cm}list scipy.\+optimize.\+anneal.\+upper = \mbox{[}3.\+0, 3.\+0\mbox{]}}\label{namespacescipy_1_1optimize_1_1anneal_a043335cd5334b15f26ffea18942a2149}
